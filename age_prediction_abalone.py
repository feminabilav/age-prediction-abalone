# -*- coding: utf-8 -*-
"""MLTerapan_Proyek_Pertama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12fVosiModfSpURYg7Xjz0W0rTheFmRV9

Dilakukan import dataset dari ICU ML
"""

!pip install ucimlrepo

from ucimlrepo import fetch_ucirepo

# fetch dataset
abalone = fetch_ucirepo(id=1)

# data (as pandas dataframes)
X = abalone.data.features
y = abalone.data.targets

# metadata
print(abalone.metadata)

# variable information
print(abalone.variables)

import numpy as np
import pandas as pd
import seaborn as sns

"""Dilihat info dari dataset, terlihat bahwa ada 8 fitur masing masing berisi 4177 dan tidak ada null, Sex bertipe object dan sisanya bertipe float"""

X.info()

X.describe()

"""Dilihat bahwa target(y) juga tidak ada yang null dan memiliki tipe int untuk menunjukkan Rings yang dimiliki Abalone"""

y.info()

"""## Exploratory Data Analysis

### Penanganan Outliers

Dilakukan pengecekan outliers pada tiap fitur dengan tipe float
"""

sns.boxplot(x=X['Length'])

sns.boxplot(x=X['Diameter'])

sns.boxplot(x=X['Height'])

sns.boxplot(x=X['Whole_weight'])

sns.boxplot(x=X['Shucked_weight'])

sns.boxplot(x=X['Viscera_weight'])

sns.boxplot(x=X['Shell_weight'])

"""Terlihat ada beberapa outliers sehingga perlu dilakukan penanganan terlebih dahulu"""

#Menggabungkan X dan y terlebih dahulu
columns = ['Length',	'Diameter',	'Height',	'Whole_weight',	'Shucked_weight',	'Viscera_weight',	'Shell_weight', 'Rings']

df = pd.concat([X,y], axis=1)
for column in columns:
  Q1 = df[column].quantile(0.25)
  Q3 = df[column].quantile(0.75)
  IQR=Q3-Q1
  df=df[~((df[column]<(Q1-1.5*IQR))|(df[column]>(Q3+1.5*IQR)))]



# Cek ukuran dataset setelah kita drop outliers
df.shape

"""Terlihat bahwa sudah ada beberapa baris outliers yang berhasil dihapus sehingga jumlah data berubah dari 4177 instances menjadi 3773 instances"""

# Pisah jadi X feature dan y target
X=df.drop(['Rings'], axis=1)
y=df['Rings']

"""### Univariate Analysis

Dilakukan univariate analysis terhadap:
- fitur numerical yaitu 'Length',	'Diameter',	'Height',	'Whole_weight',	'Shucked_weight',	'Viscera_weight',	'Shell_weight'
- fitur categorical yaitu 'Sex'
"""

numerical_features = ['Length',	'Diameter',	'Height',	'Whole_weight',	'Shucked_weight',	'Viscera_weight',	'Shell_weight']
categorical_features = ['Sex']

"""Dibuat histogram untuk setiap fitur numerical"""

import matplotlib.pyplot as plt

X[numerical_features].hist(bins=50, figsize=(20,15))
plt.show()

"""Dan juga diliat histogram untuk target"""

y.hist(bins=50, figsize=(20,15))
plt.show()

"""Dari histogram, ditemukan bahwa:
- Length dan Diameter, juga Whole_Weight, Viscera_Weight, Shucked_Weight, dan Shell Weight memiliki grafik yang mirip hal ini dapat menunjukkan korelasi yang tinggi antara fitur tersebut
- Target memiliki distribusi normal dengan kebanyakan berada pada rentang 8 sampai 10

Kemudian dilihat juga bar plot untuk categorical feature 'Sex'
"""

count = X[categorical_features].value_counts()
percent = 100*X[categorical_features].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=categorical_features, subplots=True);

"""Dapat dilihat bahwa kebanyakan data memiliki 'Sex' Male, diikuti dengan Infant, dan terakhir Female

### Multivariate Analysis

Kemudian dilakukan multivariate analysis antara target 'Rings' dengan fitur 'Sex'
"""

cat_features = X.select_dtypes(include='object').columns.to_list()

for col in cat_features:
  sns.catplot(x=col, y=y, kind="bar", dodge=False, height = 4, aspect = 3,  data=X, palette="Set3")
  plt.title("Rata-rata 'Rings' Relatif terhadap - {}".format(col))

"""Dapat dilihat bahwa Female memiliki rata-rata Rings paling tinggi, diikuti dengan Male

## Data Preparation

### One-Hot Encoding
"""

from sklearn.preprocessing import OneHotEncoder
X = pd.concat([X, pd.get_dummies(X['Sex'], prefix='Sex')],axis=1)
X.drop(['Sex'], axis=1, inplace=True)
X.replace({False: 0, True: 1}, inplace=True)
X.head()

import pandas as pd

"""### Reduksi Dimensi PCA"""

sns.pairplot(X, diag_kind = 'kde')

plt.figure(figsize=(10, 8))
correlation_matrix = pd.concat([X[numerical_features], y], axis=1).corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""Dapat dilihat dari koerlasi matrix bahwa Diameter dan Length memiliki korelasi yang sangat tinggi yaitu 0.99, karena keduanya menunjukkan ukuran dari Abalone. Sama halnya dengan fitur mengenai Whole_Weight, Shucked_Weight, Viscera_Weight, dan Shell_Weight karena mereka semua merupakan data yang berhubungan dengan berat dari Abalone

Pada proyek ini, dilakukan reduksi dimensi hanya untuk fitur 'Diameter' dan 'Length' saja seperti berikut
"""

from sklearn.decomposition import PCA

pca = PCA(n_components=2, random_state=123)
pca.fit(X[['Diameter', 'Length']])
princ_comp = pca.transform(X[['Diameter', 'Length']])

pca.explained_variance_ratio_.round(3)

"""Dapat dilihat bahwa sebanyak ~99% informasi berada pada PC pertama dan sisanya pada PC kedua"""

from sklearn.decomposition import PCA
pca = PCA(n_components=1, random_state=123)
pca.fit(X[['Diameter', 'Length']])
X['dimension'] = pca.transform(X.loc[:, ('Diameter', 'Length')]).flatten()
X.drop(['Diameter', 'Length'], axis=1, inplace=True)

"""### Train-Test-Split

Dataset dibagi menjadi data latih (train data) dan data uji (test data) dengan rasio 80:20
"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 33)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""Dilakukan standarisasi pada train data agar standar deviasi sama dengan 1 dan mean sama dengan 0"""

from sklearn.preprocessing import StandardScaler

numerical_features = ['dimension',	'Height',	'Whole_weight',	'Shucked_weight',	'Viscera_weight',	'Shell_weight']

scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

X_train[numerical_features].describe().round(4)

"""Kemudian dilakukan juga standarisasi pada test data"""

scaler = StandardScaler()
scaler.fit(X_test[numerical_features])
X_test[numerical_features] = scaler.transform(X_test.loc[:, numerical_features])
X_test[numerical_features].head()

X_test[numerical_features].describe().round(4)

"""## Model Development

Dilakukan pengembangan model menggunakan KNN, Random Forest, dan Boosting Algorithm, dan juga digunakan MSE untuk evaluasi
"""

models = pd.DataFrame(index=['knn', 'RandomForest', 'Boosting'],
                      columns=['train_mse', 'test_mse'])

"""### K-Nearest Neighbor (KNN)

Digunakan pembuatan model KNN menggunakan train data dengan n=15, kemudian dievaluasi menggunakan Mean Squared Error
"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

knn = KNeighborsRegressor(n_neighbors=15)
knn.fit(X_train, y_train)

models.loc['knn', 'train_mse'] = mean_squared_error(y_pred = knn.predict(X_train), y_true=y_train)
models.loc['knn', 'test_mse'] = mean_squared_error(y_pred = knn.predict(X_test), y_true=y_test)

"""### Random Forest

Digunakan pembuatan model Random Forest menggunakan train data dengan n_estimators=50, max_depth=8, n_jobs=-1, kemudian dievaluasi menggunakan Mean Squared Error
"""

from sklearn.ensemble import RandomForestRegressor

# buat model prediksi
RF = RandomForestRegressor(n_estimators=50, max_depth=8, random_state=55, n_jobs=-1)
RF.fit(X_train, y_train)

models.loc['RandomForest','train_mse'] = mean_squared_error(y_pred = RF.predict(X_train), y_true=y_train)
models.loc['RandomForest', 'test_mse'] = mean_squared_error(y_pred = RF.predict(X_test), y_true=y_test)

"""### Boosting Algorithm

Digunakan pembuatan model Boosting menggunakan train data dengan learning_rate=0.08, kemudian dievaluasi menggunakan Mean Squared Error
"""

from sklearn.ensemble import AdaBoostRegressor

boosting = AdaBoostRegressor(learning_rate=0.08, random_state=55)
boosting.fit(X_train, y_train)
models.loc['Boosting','train_mse'] = mean_squared_error(y_pred=boosting.predict(X_train), y_true=y_train)
models.loc['Boosting', 'test_mse'] = mean_squared_error(y_pred = boosting.predict(X_test), y_true=y_test)

"""## Hasil

Hasil dari evaluasi model dapat dilihat seperti berikut
"""

models

fig, ax = plt.subplots()
models.sort_values(by='test_mse', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Dapat dilihat bahwa hasil terbaik merupakan model Random Forest yang memiliki MSE paling sedikit pada train data dan test data. Tetapi selisih antara train_mse dan test_mse besar sehingga ada kemungkinan bahwa model mengalami over-fitting"""